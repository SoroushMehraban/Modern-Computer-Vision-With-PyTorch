{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[1, 2], [3, 4], [5, 6], [7, 8]]\n",
    "y = [[3], [7], [11], [15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(x).float()\n",
    "Y = torch.tensor(y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "X = X.to(device)\n",
    "Y = Y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x.detach().clone()\n",
    "        self.y = y.detach().clone()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        \"\"\"\n",
    "        Function to fetch a specific row.\n",
    "        :param ix: index of the row that is to be fetched from the dataset.\n",
    "        \"\"\"\n",
    "        return self.x[ix], self.y[ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important\n",
    "`.clone()` returns a copy of data and it keeps the computation graph.   \n",
    "`.detach()` returns a new view of the tensor without computation graph. (it doesn't allocate new memory for the new tensor since it's just a view)\n",
    "\n",
    "It appears that `.detach().clone()` is more efficient than `.clone().detach()`. Because by utilizing the second approach, at first we duplicate computation graph and later abandon it which is pointless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(X, Y)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True) # fetch random sample of two data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[7., 8.],\n",
      "        [5., 6.]], device='cuda:0')\n",
      "y: tensor([[15.],\n",
      "        [11.]], device='cuda:0')\n",
      "---\n",
      "x: tensor([[3., 4.],\n",
      "        [1., 2.]], device='cuda:0')\n",
      "y: tensor([[7.],\n",
      "        [3.]], device='cuda:0')\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataloader:\n",
    "    print(f\"x: {x}\")\n",
    "    print(f\"y: {y}\")\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_to_hidden_layer = nn.Linear(2, 8)\n",
    "        self.hidden_layer_activation = nn.ReLU()\n",
    "        self.hidden_to_output_layer = nn.Linear(8, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_to_hidden_layer(x)\n",
    "        x = self.hidden_layer_activation(x)\n",
    "        x = self.hidden_to_output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_net = MyNeuralNet().to(device)\n",
    "loss_function = nn.MSELoss()\n",
    "opt = SGD(my_net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5704784393310547"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_history = []\n",
    "start = time.time()\n",
    "for epoch in range(50):\n",
    "    for data in dataloader:\n",
    "        x, y = data\n",
    "        opt.zero_grad()\n",
    "        loss_value = loss_function(my_net(x), y)\n",
    "        loss_value.backward()\n",
    "        opt.step()\n",
    "        loss_history.append(float(loss_value))\n",
    "end = time.time()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on new data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x = [[10, 11]]\n",
    "val_x = torch.tensor(val_x).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20.4166]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_net(val_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
