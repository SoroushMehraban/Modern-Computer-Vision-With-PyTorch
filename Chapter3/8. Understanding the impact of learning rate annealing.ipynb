{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "data_folder = 'dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "tr_fmnist = datasets.FashionMNIST(data_folder, download=True, train=True)\n",
    "tr_images, tr_targets = tr_fmnist.data, tr_fmnist.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_fmnist = datasets.FashionMNIST(data_folder, download=True, train=False)\n",
    "val_images, val_targets = val_fmnist.data, val_fmnist.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions And Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMNISTDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        x = x.float() / 255\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        self.x, self.y = x, y\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        return self.x[ix].to(device), self.y[ix].to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(learning_rate):\n",
    "    model = nn.Sequential(\n",
    "                            nn.Linear(28 * 28, 1000),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(1000, 10)\n",
    "                        ).to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return model, optimizer, loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(x, y, model, optimizer, loss_function):\n",
    "    model.train()\n",
    "    prediction = model(x)\n",
    "    batch_loss = loss_function(prediction, y)\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return batch_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x, y, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(x)\n",
    "    max_values, argmaxes = prediction.max(-1)\n",
    "    is_correct = argmaxes == y\n",
    "    return is_correct.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size):\n",
    "    train_data = FMNISTDataset(tr_images, tr_targets)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    val_data = FMNISTDataset(val_images, val_targets)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=val_images.shape[0], shuffle=False)\n",
    "    \n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def val_loss(x, y, model, loss_function):\n",
    "    model.eval()\n",
    "    prediction = model(x)\n",
    "    val_loss = loss_function(prediction, y)\n",
    "    return val_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Learning Rate Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_datalader = get_data(batch_size=32)\n",
    "model, optimizer, loss_function = get_model(learning_rate=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ReduceLROnPlateau(optimizer,\n",
    "                              factor=0.5, # after patience number of epochs, lr *= factor\n",
    "                              patience=0,\n",
    "                              threshold=0.001, # threshold that we consider change if it's larger than this\n",
    "                              verbose=True,\n",
    "                              min_lr=1e-5,  # Specify that after lr *= factor, lr should not be smaller than min_lr\n",
    "                              threshold_mode='abs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch: 0\n",
      "Current epoch: 1\n",
      "Current epoch: 2\n",
      "Current epoch: 3\n",
      "Current epoch: 4\n",
      "Epoch     5: reducing learning rate of group 0 to 5.0000e-03.\n",
      "Current epoch: 5\n",
      "Current epoch: 6\n",
      "Current epoch: 7\n",
      "Epoch     8: reducing learning rate of group 0 to 2.5000e-03.\n",
      "Current epoch: 8\n",
      "Current epoch: 9\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-03.\n",
      "Current epoch: 10\n",
      "Current epoch: 11\n",
      "Epoch    12: reducing learning rate of group 0 to 6.2500e-04.\n",
      "Current epoch: 12\n",
      "Epoch    13: reducing learning rate of group 0 to 3.1250e-04.\n",
      "Current epoch: 13\n",
      "Epoch    14: reducing learning rate of group 0 to 1.5625e-04.\n",
      "Current epoch: 14\n",
      "Epoch    15: reducing learning rate of group 0 to 7.8125e-05.\n",
      "Current epoch: 15\n",
      "Epoch    16: reducing learning rate of group 0 to 3.9063e-05.\n",
      "Current epoch: 16\n",
      "Epoch    17: reducing learning rate of group 0 to 1.9531e-05.\n",
      "Current epoch: 17\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Current epoch: 18\n",
      "Current epoch: 19\n",
      "Current epoch: 20\n",
      "Current epoch: 21\n",
      "Current epoch: 22\n",
      "Current epoch: 23\n",
      "Current epoch: 24\n",
      "Current epoch: 25\n",
      "Current epoch: 26\n",
      "Current epoch: 27\n",
      "Current epoch: 28\n",
      "Current epoch: 29\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_accuracies = [], []\n",
    "val_losses, val_accuracies = [], []\n",
    "for epoch in range(30):\n",
    "    print(f\"Current epoch: {epoch}\")\n",
    "    \n",
    "    train_epoch_losses, train_epoch_accuracies = [], []\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        x, y = batch\n",
    "        batch_loss = train_batch(x, y, model, optimizer, loss_function)\n",
    "        train_epoch_losses.append(batch_loss)\n",
    "    train_epoch_loss = np.mean(train_epoch_losses)\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        x, y = batch\n",
    "        is_correct = accuracy(x, y, model)\n",
    "        train_epoch_accuracies.extend(is_correct)\n",
    "    train_epoch_accuracy = np.mean(train_epoch_accuracies)\n",
    "    \n",
    "    for batch in val_datalader:\n",
    "        x, y = batch\n",
    "        val_is_correct = accuracy(x, y, model)\n",
    "        val_epoch_loss = val_loss(x, y, model, loss_function)\n",
    "        scheduler.step(val_epoch_loss)\n",
    "    val_epoch_accuracy = np.mean(val_is_correct)\n",
    "    \n",
    "    train_losses.append(train_epoch_loss)\n",
    "    train_accuracies.append(train_epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracies.append(val_epoch_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
